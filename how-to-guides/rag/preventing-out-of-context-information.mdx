---
title: Preventing Out of Context Information
description: Learn how to prevent out of context information from being generated by your AI models.
---

If a model generates responses that include information not found in the retrieved context, it introduces closed-domain hallucinations. This means the model is making up facts rather than relying on retrieved information, leading to misinformation and reduced trust.

### Example of the Problem
**User Query:** "What year was the Eiffel Tower completed?" 

**Retrieved Context:** "The Eiffel Tower is an iron lattice tower located in Paris, France. It was designed by Gustave Eiffel." 

**Model Response:** "The Eiffel Tower was completed in 1889 and is the most visited paid monument in the world."

## What Went Wrong?

* **What We Did Wrong:**  
  * Retrieved documents contained irrelevant information.  
  * The model overgeneralized or extrapolated beyond what was retrieved.  
  * The retrieval pipeline was returning too many noisy or loosely related chunks.  
* **How It Showed Up in Metrics:**  
  * **Low Context Adherence**: The model included information not present in the retrieved documents.  
  * **High Chunk Attribution but Low Chunk Utilization**: The model referenced retrieved data but incorporated only small portions of it.

## Improvements and Solutions

<Note>
Skim through each of these solutions before choosing the best one for your situation!
</Note>

<Steps>
<Step title="Setting Up the Environment">
First, let's set up our environment with the necessary imports and configurations:

```python
import os
from dotenv import load_dotenv
from galileo import openai, log, galileo_context
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
import questionary

load_dotenv()

# Initialize console for rich output
console = Console()

# Check if Galileo logging is enabled
logging_enabled = os.environ.get("GALILEO_API_KEY") is not None

galileo_context.init(project="out-of-context", log_stream="dev")

# Initialize OpenAI client
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
```

This setup includes:
* Loading environment variables for API keys
* Initializing the console for rich output formatting
* Setting up Galileo logging for tracking operations
* Creating an OpenAI client for model interactions
</Step>

<Step title="Understanding the Document Retriever">
The document retriever is designed to demonstrate how incomplete context can lead to out-of-context information:

```python
@log(span_type="retriever")
def retrieve_documents(query: str):
    """
    Simulated document retrieval that intentionally returns incomplete information
    to demonstrate the out-of-context problem.
    """
    # Dictionary of queries and their intentionally incomplete contexts
    incomplete_contexts = {
        "eiffel tower": [
            {
                "content": "The Eiffel Tower is an iron lattice tower located in Paris, France. It was designed by Gustave Eiffel.",
                "metadata": {
                    "id": "doc1",
                    "source": "travel_guide",
                    "category": "landmarks",
                    "relevance": "high"
                }
            }
        ],
        # Additional predefined queries...
    }
    
    # Default case for queries not in our predefined list
    default_docs = [
        {
            "content": "This is a generic response with limited information about the query topic.",
            "metadata": {
                "id": "default_doc",
                "source": "general_knowledge",
                "category": "miscellaneous",
                "relevance": "low"
            }
        }
    ]
    
    # Find the most relevant predefined query
    for key in incomplete_contexts:
        if key in query.lower():
            return incomplete_contexts[key]
    
    return default_docs
```

Key points about the retriever:
* It simulates real-world document retrieval with intentionally incomplete information
* Uses predefined contexts to demonstrate the out-of-context problem
* Includes metadata for tracking document sources and relevance
</Step>

<Step title="Demonstrating the Problem">
Let's look at how a weak prompt can lead to out-of-context information:

```python
@log(name="rag_with_hallucination")
def rag_with_hallucination(query: str):
    """
    RAG implementation that demonstrates the out-of-context problem by using
    a system prompt that doesn't properly constrain the model.
    """
    documents = retrieve_documents(query)
    
    # Format documents for better readability in the prompt
    formatted_docs = ""
    for i, doc in enumerate(documents):
        formatted_docs += f"Document {i+1} (Source: {doc['metadata']['source']}):\n{doc['content']}\n\n"

    # This prompt doesn't strongly constrain the model
    weak_prompt = f"""
    Answer the following question based on the context provided.
    
    Question: {query}

    Context:
    {formatted_docs}
    """

    try:
        console.print("[bold blue]Generating answer (prone to out-of-context information)...[/bold blue]")
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": weak_prompt}
            ],
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error generating response: {str(e)}"
```

Problems with this approach:
* The weak prompt doesn't explicitly constrain the model
* The system message is too generic
* No explicit instruction to avoid using external knowledge
</Step>

<Step title="Implementing the Solution">
Now, let's see how to prevent out-of-context information with a stronger prompt:

```python
@log(name="rag_with_constraint")
def rag_with_constraint(query: str):
    """
    RAG implementation that demonstrates how to mitigate the out-of-context problem
    by using a stronger system prompt and explicit instructions.
    """
    documents = retrieve_documents(query)
    
    # Format documents for better readability in the prompt
    formatted_docs = ""
    for i, doc in enumerate(documents):
        formatted_docs += f"Document {i+1} (Source: {doc['metadata']['source']}):\n{doc['content']}\n\n"

    # This prompt strongly constrains the model
    strong_prompt = f"""
    Answer the following question based STRICTLY on the context provided. 
    If the information needed to answer the question is not explicitly contained in the context, 
    respond with: "I don't have enough information in the provided context to answer this question."
    
    DO NOT use any knowledge outside of the provided context.
    
    Question: {query}

    Context:
    {formatted_docs}
    """

    try:
        console.print("[bold green]Generating answer (constrained to context)...[/bold green]")
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that ONLY answers based on the provided context. Never use external knowledge."},
                {"role": "user", "content": strong_prompt}
            ],
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error generating response: {str(e)}"
```

Key improvements:
* Explicit instruction to use only provided context
* Clear directive to acknowledge when information is missing
* Stronger system message that reinforces context adherence
</Step>

<Step title="Running the Interactive Demo">
The main function provides an interactive way to test and compare both approaches:

```python
@log
def main():
    console.print(Panel.fit(
        "[bold]Out-of-Context RAG Demo[/bold]\nThis demo shows how RAG systems can generate out-of-context information and how to prevent it.",
        title="Galileo RAG Challenge: Out-of-Context Information",
        border_style="red"
    ))
    
    # Environment checks
    if logging_enabled:
        console.print("[green]✅ Galileo logging is enabled[/green]")
    else:
        console.print("[yellow]⚠️ Galileo logging is disabled[/yellow]")
    
    api_key = os.environ.get("OPENAI_API_KEY")
    if api_key:
        console.print("[green]✅ OpenAI API Key is set[/green]")
    else:
        console.print("[red]❌ OpenAI API Key is missing[/red]")
        return
    
    # Example queries that demonstrate the problem
    suggested_queries = [
        "When was the Eiffel Tower completed?",
        "Who created the Python language and when?",
        "What are the main effects of climate change?",
        "When was artificial intelligence first developed?",
        "How many qubits are in the most powerful quantum computer?"
    ]
```

The demo allows users to:
* Test predefined queries that highlight the problem
* Compare responses from both approaches
* See the effectiveness of context constraints
</Step>

<Step title="Analyzing the Results">
When running the demo, you'll notice:

* **Unconstrained Responses**: May include information not present in the context
* **Constrained Responses**: Strictly adhere to provided information
* **Completeness vs. Accuracy**: Trade-off between complete answers and factual accuracy

Here's an example comparison:

Query: "When was the Eiffel Tower completed?"

Unconstrained Response:
```
The Eiffel Tower was completed in 1889 for the World's Fair in Paris.
```

Constrained Response:
```
I don't have enough information in the provided context to answer this question. The context only mentions that the Eiffel Tower is an iron lattice tower in Paris and was designed by Gustave Eiffel.
```

The constrained response demonstrates better adherence to the available context, even though it provides less information.
</Step>

<Step title="Best Practices and Recommendations">
To prevent out-of-context information in your RAG system:

1. **Strong Prompting**:
   * Be explicit about using only provided context
   * Include clear instructions for handling missing information
   * Use system messages that reinforce context adherence

2. **Context Management**:
   * Ensure retrieved documents are relevant and complete
   * Include metadata for tracking document sources
   * Monitor and log context utilization

3. **Response Validation**:
   * Compare responses against provided context
   * Track and measure context adherence
   * Use Galileo metrics to monitor performance

4. **User Experience**:
   * Clearly communicate when information is limited
   * Provide transparent source attribution
   * Balance completeness with accuracy
</Step>
</Steps>