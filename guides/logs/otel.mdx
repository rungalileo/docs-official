---
title: "Logging to Galileo using OpenTelemetry and OpenInference (Python)"
description: "This guide explains how to send OpenTelemetry (OTEL) traces to Galileo using OpenInference. Configure the endpoint and headers to get started quickly."
icon: "python"
---

## Installation

Install the required OpenTelemetry and OpenInference dependencies:

```bash
pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation
```

> _Tested with:_
> - `opentelemetry-api==1.23.0`
> - `opentelemetry-sdk==1.23.0`
> - `openinference-instrumentation-langchain==0.3.1`

---

## Authentication

1. [Generate an API key](https://app.galileo.ai/settings/api-keys) from your Galileo account.  
2. Copy your **project name** and **log stream name** from the Galileo console.

Set the authentication headers in your environment:

```python
import os

headers = {
    "Galileo-API-Key": "<YOUR_API_KEY>",           # Replace with your actual API key
    "project_name": "<YOUR_PROJECT_NAME>",         # Replace with your project name
    "log_stream_name": "<YOUR_RUN_NAME>",          # Must match the log stream name in Galileo
}

# Format headers as a comma-separated key=value string
# Example: Galileo-API-Key=abc123,project_name=my_project,log_stream_name=my_run
os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS'] = ",".join([f"{k}={v}" for k, v in headers.items()])
```

---

## OpenTelemetry Setup

Place this setup at the top of your script, before initializing your LLM or pipeline framework:

```python
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry import trace as trace_api
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import ConsoleSpanExporter
from openinference.instrumentation.langchain import LangChainInstrumentor

# OTEL exporter endpoint
endpoint = "http://api.galileo.ai/otel/traces"

# Configure OTEL tracer provider
tracer_provider = trace_sdk.TracerProvider()

# Sends traces to Galileo
tracer_provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint)))

# Optional: Prints traces to console for local debugging
tracer_provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))

trace_api.set_tracer_provider(tracer_provider=tracer_provider)

# Instrument LangChain (or replace with your framework's instrumentor)
LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
```

> **Note:** Apply instrumentation **before** importing or initializing your framework (e.g., LangChain, OpenAI).

---

## Full Working Example

Check out a complete working example on Colab:  
ðŸ”— [OpenTelemetry + Galileo Integration Notebook](https://colab.research.google.com/drive/14c1-UVmOVBQt_CRGnOlTEcvgf7yBLiPB)

---

## Summary

To enable OTEL logging with Galileo:

1. Set the OTEL headers using your API key, project name, and log stream name.
2. Set the exporter endpoint: `http://api.galileo.ai/otel/traces`
3. Register your tracer provider and processors.
4. Import and apply the OpenInference instrumentor(s) before initializing your application.

---

## Supported Instrumentors

These can be used individually or in combination, depending on your framework:

- [OpenAI](https://pypi.org/project/openinference-instrumentation-openai/)
- [LLama Index](https://pypi.org/project/openinference-instrumentation-llama-index/)
- [DSPy](https://pypi.org/project/openinference-instrumentation-dspy/)
- [Bedrock](https://pypi.org/project/openinference-instrumentation-bedrock/)
- [Langchain](https://pypi.org/project/openinference-instrumentation-langchain/)
- [Mistral](https://pypi.org/project/openinference-instrumentation-mistralai/)
- [Guardrails](https://pypi.org/project/openinference-instrumentation-guardrails/)
- [Vertex AI](https://pypi.org/project/openinference-instrumentation-vertexai/)
- [CrewAI](https://pypi.org/project/openinference-instrumentation-crewai/)
- [Haystack](https://pypi.org/project/openinference-instrumentation-haystack/)
- [LiteLLM](https://pypi.org/project/openinference-instrumentation-litellm/)
- [Groq](https://pypi.org/project/openinference-instrumentation-groq/)
- [Instructor](https://pypi.org/project/openinference-instrumentation-instructor/)
- [Anthropic](https://pypi.org/project/openinference-instrumentation-anthropic/)

---

## FAQ

### How do I create a project?

Log in to the Galileo console and create a new logging project. After creation, copy the **project name** and **log stream name** from the interface.

---

### Which instrumentors should I import?

Choose the instrumentor(s) matching your framework:

```python
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
from openinference.instrumentation.dspy import DSPyInstrumentor
from openinference.instrumentation.bedrock import BedrockInstrumentor
from openinference.instrumentation.langchain import LangChainInstrumentor
from openinference.instrumentation.mistralai import MistralAIInstrumentor
from openinference.instrumentation.guardrails import GuardrailsInstrumentor
from openinference.instrumentation.vertexai import VertexAIInstrumentor
from openinference.instrumentation.crewai import CrewAIInstrumentor
from openinference.instrumentation.haystack import HaystackInstrumentor
from openinference.instrumentation.litellm import LiteLLMInstrumentor
from openinference.instrumentation.groq import GroqInstrumentor
from openinference.instrumentation.instructor import InstructorInstrumentor
from openinference.instrumentation.anthropic import AnthropicInstrumentor
```

#### Instructor Example with Client:
```python
from openinference.instrumentation.instructor import InstructorInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor
from openai import OpenAI
import instructor

client = instructor.from_openai(OpenAI())
```

---

### What is the correct OTEL endpoint?

Use the Galileo API base URL with the `/otel/traces` path:

```
http://api.galileo.ai/otel/traces
```

If your console URL is `https://console.galileo.ai`, the endpoint becomes `http://api.galileo.ai/otel/traces`.

---

### How do I verify itâ€™s working?

- Use `ConsoleSpanExporter` to see local trace output.
- Check the Galileo dashboard for incoming traces.
- Add a test span like this:

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span("test-span"):
    print("Tracing is active")
```

---

### Troubleshooting

- âœ… Confirm your API key, project name, and log stream name are correct.
- âœ… Make sure the environment variable is correctly formatted.
- âœ… Ensure the endpoint is accessible from your environment.
- âœ… Instrument your framework before using it.
- âœ… Use `print(os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS'])` to debug your header string.

---

## Resources

- [OpenTelemetry Python Documentation](https://opentelemetry.io/docs/instrumentation/python/)
- [OpenInference GitHub Repository](https://github.com/braintrustdata/openinference)
