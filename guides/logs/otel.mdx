---
title: "Logging to Galileo using OpenTelemetry and OpenInference (Python)"
description: "This guide explains how to send OpenTelemetry (OTEL) traces to Galileo using OpenInference. Configure the endpoint and headers to get started quickly."
icon: "python"
---

## Installation

Install the required OpenTelemetry and OpenInference dependencies:

```bash
pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation
```

> _Tested with:_
> - `opentelemetry-api==1.23.0`
> - `opentelemetry-sdk==1.23.0`
> - `openinference-instrumentation-langchain==0.3.1`

---

## Authentication

1. [Generate an API key](https://app.galileo.ai/settings/api-keys) from your Galileo account.  
2. Copy your **project name** and **log stream name** from the Galileo console.

Set the authentication headers in your environment:

```python
import os

headers = {
    "Galileo-API-Key": "<YOUR_API_KEY>",           # Replace with your actual API key
    "project": "<YOUR_PROJECT_NAME>",         # Replace with your project name
    "logstream": "<YOUR_RUN_NAME>",          # Must match the log stream name in Galileo
}

# Format headers as a comma-separated key=value string
# Example: Galileo-API-Key=abc123,project=my_project,logstream=my_run
os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS'] = ",".join([f"{k}={v}" for k, v in headers.items()])
```

---

## OpenTelemetry Setup

Place this setup at the top of your script, before initializing your LLM or pipeline framework:

```python
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry import trace as trace_api
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import ConsoleSpanExporter
from openinference.instrumentation.langchain import LangChainInstrumentor

# OTEL exporter endpoint
endpoint = "http://api.galileo.ai/otel/traces"

# Configure OTEL tracer provider
tracer_provider = trace_sdk.TracerProvider()

# Sends traces to Galileo
tracer_provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint)))

# Optional: Prints traces to console for local debugging
tracer_provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))

trace_api.set_tracer_provider(tracer_provider=tracer_provider)

# Instrument LangChain (or replace with your framework's instrumentor)
LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
```

> **Note:** Apply instrumentation **before** importing or initializing your framework (e.g., LangChain, OpenAI).

---

## Full Working Example

Check out a complete working example on Colab:  
ðŸ”— [OpenTelemetry + Galileo Integration Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_LangChain_Notebook.ipynb)

---

## Summary

To enable OTEL logging with Galileo:

1. Set the OTEL headers using your API key, project name, and log stream name.
2. Set the exporter endpoint: `http://api.galileo.ai/otel/traces`
3. Register your tracer provider and processors.
4. Import and apply the OpenInference instrumentor(s) before initializing your application.

---

## Supported Instrumentors

These can be used individually or in combination, depending on your framework:

- [OpenAI](https://pypi.org/project/openinference-instrumentation-openai/)
- [LLama Index](https://pypi.org/project/openinference-instrumentation-llama-index/)
- [DSPy](https://pypi.org/project/openinference-instrumentation-dspy/)
- [Bedrock](https://pypi.org/project/openinference-instrumentation-bedrock/)
- [Langchain](https://pypi.org/project/openinference-instrumentation-langchain/)
- [Mistral](https://pypi.org/project/openinference-instrumentation-mistralai/)
- [Guardrails](https://pypi.org/project/openinference-instrumentation-guardrails/)
- [Vertex AI](https://pypi.org/project/openinference-instrumentation-vertexai/)
- [CrewAI](https://pypi.org/project/openinference-instrumentation-crewai/)
- [Haystack](https://pypi.org/project/openinference-instrumentation-haystack/)
- [LiteLLM](https://pypi.org/project/openinference-instrumentation-litellm/)
- [Groq](https://pypi.org/project/openinference-instrumentation-groq/)
- [Instructor](https://pypi.org/project/openinference-instrumentation-instructor/)
- [Anthropic](https://pypi.org/project/openinference-instrumentation-anthropic/)

---

## FAQ

### How do I create a project?

Log in to the Galileo console and create a new logging project. After creation, copy the **project name** and **log stream name** from the interface.

---

### Which instrumentors should I import?

Choose the instrumentor(s) matching your framework:

```python
# OpenAI
from openinference.instrumentation.openai import OpenAIInstrumentor

# LlamaIndex
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor

# DSPy
from openinference.instrumentation.dspy import DSPyInstrumentor

# Bedrock
from openinference.instrumentation.bedrock import BedrockInstrumentor

# LangChain
from openinference.instrumentation.langchain import LangChainInstrumentor

# Mistral AI
from openinference.instrumentation.mistralai import MistralAIInstrumentor

# Guardrails
from openinference.instrumentation.guardrails import GuardrailsInstrumentor

# Vertex AI
from openinference.instrumentation.vertexai import VertexAIInstrumentor

# CrewAI
from openinference.instrumentation.crewai import CrewAIInstrumentor
from openinference.instrumentation.langchain import LangChainInstrumentor

# Haystack
from openinference.instrumentation.haystack import HaystackInstrumentor

# LiteLLM
from openinference.instrumentation.litellm import LiteLLMInstrumentor

# Groq
from openinference.instrumentation.groq import GroqInstrumentor

# Instructor
from openinference.instrumentation.instructor import InstructorInstrumentor

# Anthropic
from openinference.instrumentation.anthropic import AnthropicInstrumentor
```

#### Instructor Example with Client:
```python
from openinference.instrumentation.instructor import InstructorInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor
from openai import OpenAI
import instructor

client = instructor.from_openai(OpenAI())
```

---

### What is the correct OTEL endpoint?

Use the Galileo API base URL with the `/otel/traces` path:

```
http://api.galileo.ai/otel/traces
```

If your console URL is `https://console.galileo.ai`, the endpoint becomes `http://api.galileo.ai/otel/traces`.

---

### How do I verify itâ€™s working?

- Use `ConsoleSpanExporter` to see local trace output.
- Check the Galileo dashboard for incoming traces.
- Add a test span like this:

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span("test-span"):
    print("Tracing is active")
```

---

### Troubleshooting

- âœ… Confirm your API key, project name, and log stream name are correct.
- âœ… Make sure the environment variable is correctly formatted.
- âœ… Ensure the endpoint is accessible from your environment.
- âœ… Instrument your framework before using it.
- âœ… Restart environment after changing environment variables.
- âœ… Use `print(os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS'])` to debug your header string.

---

## Resources

- [OpenTelemetry Python Documentation](https://opentelemetry.io/docs/instrumentation/python/)
- [OpenInference GitHub Repository](https://github.com/braintrustdata/openinference)

## Notebooks

ðŸ”— [Anthropic + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_Anthropic_Notebook.ipynb)
ðŸ”— [Crew AI + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_Crew_AI_Notebook.ipynb)
ðŸ”— [DSPY + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_DSPY_Notebook.ipynb)
ðŸ”— [Groq + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_GROQ_Notebook.ipynb)
ðŸ”— [LLamaIndex + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_LLama_Index_Notebook.ipynb)
ðŸ”— [LangChain + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_LangChain_Notebook.ipynb)
ðŸ”— [LiteLLM + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_LiteLLM_Notebook.ipynb)
ðŸ”— [Mistral + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_Mistral_Notebook.ipynb)
ðŸ”— [OpenAI + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_OpenAI_Notebook.ipynb)
ðŸ”— [Vertexai + Galileo + Otel Notebook](https://colab.research.google.com/github/rungalileo/examples/blob/main/examples/open-telemetry/Galileo_OTEL_Vertexai_Notebook.ipynb)