---
title: Experiments
icon: "flask"
---

import SnippetPromptTemplate from "/snippets/code/typescript/sdk/experiments/prompt-template.mdx";
import SnippetRunnerFunction from "/snippets/code/typescript/sdk/experiments/runner-function.mdx";
import SnippetCustomDataset from "/snippets/code/typescript/sdk/experiments/custom-dataset.mdx";
import SnippetCustomMetrics from "/snippets/code/typescript/sdk/experiments/custom-metrics.mdx";
import SnippetApiReferenceRun from "/snippets/code/typescript/sdk/experiments/api-reference-run.mdx";
import SnippetApiReferenceCreatePrompt from "/snippets/code/typescript/sdk/experiments/api-reference-create-prompt.mdx";

Galileo Experiments allow you to evaluate and improve your LLM applications by running tests against datasets and measuring performance using various metrics.

<Note>
  The examples below assume you have a [dataset created](./datasets) first.

</Note>

## Running an Experiment with a Prompt Template

The simplest way to get started is by using a prompt template:

<CodeGroup>
  <SnippetPromptTemplate />
</CodeGroup>

## Running an Experiment with a Runner Function

For more complex scenarios, you can use a runner function:

<CodeGroup>
  <SnippetRunnerFunction />
</CodeGroup>

## Running an Experiment with a Custom Dataset

When you need to test specific scenarios:

<CodeGroup>
  <SnippetCustomDataset />
</CodeGroup>

## Running an Experiment with Custom Metrics

For sophisticated evaluation needs:

<CodeGroup>
  <SnippetCustomMetrics />
</CodeGroup>
