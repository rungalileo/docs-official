---
title: Python SDK Reference
icon: "python"
---

## Introduction

This document will cover the design and developer experience of the Python client library for Galileo 2.0.

## Initialization/Authentication

```env
# Scoped to an Organization
GALILEO_API_KEY=...

# Optional, if you're not using the multi-tenant cluster
# GALILEO_CONSOLE_URL=https://console.experimental-aws.rungalileo.io/

# Optional, set a default Project
GALILEO_PROJECT=...
# Optional, set a default Log Stream
GALILEO_LOG_STREAM=...
```

## Logging

The simplest way to get started is to use our OpenAI client wrapper. This example will automatically produce a single-span trace in the Logstream UI:

```python
from galileo.openai import openai


def llm_call(input):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about {input}"}
        ],
    ).choices[0].message.content

response = llm_call("a sailor")
print(response)
```

By default, any traces that are created during a request are automatically flushed (i.e. uploaded to Galileo) when the request terminates. To flush traces prior to that, you can use the context manager or the GalileoLogger (outlined in a section below).

The `@log` decorator is used to capture the inputs and outputs of a function as a span. By default, a workflow span is created when span_type isn't specified. Here are the different span types:

- **Workflow**: A span that can have child spans, useful for nesting several child spans to denote a thread within a trace. If you add the `@log` decorator to a parent method, and calls that are made within that scope are automatically logged in the same trace.
- **Llm**: Captures the input, output, and settings of an LLM call. This span gets automatically created when our client library wrappers (OpenAI and Anthropic) are used. Cannot have nested children.
- **Retriever**: Contains the output documents of a retrieval operation.
- **Tool**: Captures the input and output of a tool call. Used to decorate functions that are invoked as tools.

This example will create a trace with a workflow span and two nested llm spans:

```python
@log
def my_wrapper_function(input):
	first_result = llm_call(input)
	second_result = another_llm_call(first_result)
	return second_result

response = my_wrapper_function("write an essay about the Roman Empire")
print(response)
```

This example shows how to mark a tool function:

```python
@log(span_type="tool")
def tool_function(arg1: str, arg2: str):
	...
	return result
```

Logging a retriever. If the output of the function is an array, it will automatically capture it as documents in the span.

```python
@log(span_type="retriever")
def retriever_function(input: str):
	...
	return documents
```

## The context manager: galileo_context()

The context manager can be useful for a few things:
- Automatically starting a trace and ensuring anything that happens in its scope is logged as a span within the trace.
- For long running app runtimes like Streamlit, the request never terminates. You can use the context manager to start a trace and ensure that traces are flushed when the manager exits.
- You might want to route a part of your app to a different Project or Log Stream. You can use the context manager to set the trace scope.

Using the context manager to create a trace with a nested LLM span (which is automatically flushed when the manager exits):

```python
from galileo.openai import openai
from galileo import galileo_context

def llm_call(input):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about {input}"}
        ],
    ).choices[0].message.content

with galileo_context():
	print(llm_call("birds"))
```

Using the context manager to route traces to a specific project and log stream:

```python
from galileo.openai import openai
from galileo import log, galileo_context

def llm_call(input):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about {input}"}
        ],
    ).choices[0].message.content

with galileo_context(project="my-project", log_stream="my-log-stream"):
	print(llm_call("birds"))
```

## Additional Documentation

For more detailed information on specific topics, please refer to the following pages:

- [Langchain Integration](./langchain) - Learn how to integrate Galileo with Langchain
- [Pure Invocation](./pure-invocation) - Direct usage of the GalileoLogger
- [Prompts](./prompts) - Creating and using prompt templates
- [Datasets](./datasets) - Working with datasets in Galileo
- [Experimentation](./experimentation) - Running experiments and evaluations
