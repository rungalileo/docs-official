---
title: Experiments
icon: "flask"
description: Run experiments with multiple datapoints 
---

Experiments in Galileo allow you to evaluate and compare different prompts, models, and configurations using datasets. This helps you identify the best approach for your specific use case.

## Running Experiments with Prompt Templates

You can run experiments using prompt templates and datasets:

```python
from galileo.datasets import get_dataset
from galileo.experiments import run_experiment
from galileo.prompts import get_prompt_template

# Get an existing prompt template
prompt = get_prompt_template(
    project="my-project",
    name="storyteller-prompt"
)

# Run an experiment with the prompt template
results = run_experiment(
    "my-experiment",
    dataset=get_dataset(name="storyteller-dataset"),
    prompt=prompt,
    metrics=["correctness"],
    project="my-project",
)
```

## Running Experiments with Custom Functions

You can also run experiments with custom functions that use the OpenAI wrapper:

```python
from galileo.datasets import get_dataset
from galileo.experiments import run_experiment
from galileo import log, openai
import os

client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

@log
def llm_call(data):
    return client.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about {data['input']}"}
        ],
    ).choices[0].message.content

# Get an existing dataset
dataset = get_dataset(name="storyteller-dataset")

# Run an experiment with the custom function
results = run_experiment(
    "my-experiment",
    dataset=dataset,
    function=llm_call,
    metrics=["correctness"],
    project="my-project",
)
```

## Creating and Using Experiments

You can create, get, and list experiments programmatically:

```python
from galileo.experiments import create_experiment, get_experiment, get_experiments

# Create a new experiment
experiment = create_experiment(
    project_id="my-project-id",
    experiment_name="my-experiment"
)

# Get an existing experiment
experiment = get_experiment(
    project_id="my-project-id",
    experiment_name="my-experiment"
)

# List all experiments in a project
experiments = get_experiments(project_id="my-project-id")
```

## Creating a New Prompt Template for Experiments

You can create a new prompt template if it doesn't exist and use it in an experiment:

```python
from galileo.datasets import get_dataset
from galileo.experiments import run_experiment
from galileo.prompts import get_prompt_template, create_prompt_template
from galileo.resources.models import Message, MessageRole

# Try to get an existing prompt template
prompt = get_prompt_template(
    project="my-project",
    name="storyteller-prompt"
)

# If the prompt doesn't exist, create it
if prompt is None:
    prompt = create_prompt_template(
        name="storyteller-prompt",
        project="my-project",
        messages=[
            Message(role=MessageRole.SYSTEM, content="You are a great storyteller."),
            Message(role=MessageRole.USER, content="Please write a short story about the following topic: {topic}")
        ]
    )

# Run an experiment with the prompt template
results = run_experiment(
    "my-experiment",
    dataset=get_dataset(name="storyteller-dataset"),
    prompt=prompt,
    metrics=["correctness"],
    project="my-project",
)
```

## Custom Metrics for Experiments

You can use custom metrics to evaluate your experiments:

```python
from galileo.datasets import get_dataset
from galileo.experiments import run_experiment
from galileo import log, openai
import os

client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

@log
def llm_call(data):
    return client.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about '{data['input']}' and make it sound like a human wrote it."}
        ],
    ).choices[0].message.content

def check_for_delve(input, output, expected) -> int:
    return 1 if "delve" not in output else 0

# Get an existing dataset
dataset = get_dataset(name="storyteller-dataset")

# Run an experiment with the custom function and metric
results = run_experiment(
    "my-experiment",
    dataset=dataset,
    function=llm_call,
    metrics=[check_for_delve],
    project="my-project",
)
```

## Best Practices

1. **Use consistent datasets**: Use the same dataset when comparing different prompts or models to ensure fair comparisons.

2. **Test multiple variations**: Run experiments with different prompt variations to find the best approach.

3. **Use appropriate metrics**: Choose metrics that are relevant to your specific use case.

4. **Start small**: Begin with a small dataset to quickly iterate and refine your approach before scaling up.

5. **Document your experiments**: Keep track of what you're testing and why to make it easier to interpret results.

## Related Resources

- [Datasets](./experimentation/datasets) - Creating and managing datasets for experiments
- [Prompts](./prompts) - Creating and using prompt templates