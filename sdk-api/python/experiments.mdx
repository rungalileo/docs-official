---
title: Experiments
icon: "flask"
description: Run experiments with multiple datapoints 
---

## Working with Prompts

```python
from galileo.prompts import get_prompt
from galileo.experiments import run_experiment
from galileo.datasets import get_dataset
from galileo.scorers import correctness

prompt = get_prompt(
	name="storyteller-prompt",
	version="latest"
)

results = run_experiment(
	dataset=get_dataset(name="storyteller-dataset"),
	prompt,
	metrics=["correctness"],
	project="my-project",
	tags=[],
)
```

## Running Experiments with Existing Datasets

```python
from galileo.experiments import run_experiment
from galileo.datasets import get_dataset
from galileo import log, openai

@log
def llm_call(data):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about {data.input}"}
        ],
    ).choices[0].message.content

dataset = get_dataset(name="storyteller-dataset")

results = run_experiment(
	dataset,
	function=llm_call,
	metrics=["correctness"],
	concurrency=1, # number of rows to execute in parallel
	project="my-project",
	tags=[],
)
```

## Custom Dataset Evaluation

```python
from galileo.experiments import run_experiment
from galileo.scorers import correctness
from galileo import log, openai

@log
def llm_call(data):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a geography expert"},
          {"role": "user", "content": f"Which continent does the following country belong to: {data.input}"}
        ],
    ).choices[0].message.content

results = run_experiment(
	dataset=lambda: [
        {
            "input": "Spain",
            "expected": "Europe",
        },
    ],
	function=llm_call,
	metrics=["correctness"],
	concurrency=1, # number of rows to execute in parallel
	project="my-project",
	tags=[],
)
```

## Custom Metrics for Deep Analysis

```python
from galileo.experiments import run_experiment
from galileo.datasets import get_dataset
from galileo import log, openai

@log
def llm_call(data):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about '{data.input}' and make it sound like a human wrote it."}
        ],
    ).choices[0].message.content

def check_for_delve(input, output, expected) -> int:
	return 1 if "delve" not in input else 0

dataset = get_dataset(name="storyteller-dataset")

results = run_experiment(
	dataset,
	function=llm_call,
	metrics=[check_for_delve],
	concurrency=2,
	project="my-project",
	tags=[],
)
```

Each of these experimentation approaches fits into different stages of your development and testing workflow. As you progress from simple prompt testing to sophisticated custom metrics, Galileo's experimentation framework provides the tools you need to gather insights and improve your application's performance at every level of complexity.
