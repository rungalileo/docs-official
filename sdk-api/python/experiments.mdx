---
title: Experiments
icon: "flask"
description: Run experiments with multiple datapoints 
---

Experiments in Galileo allow you to evaluate and compare different prompts, models, and configurations using datasets. This helps you identify the best approach for your specific use case.

## Running Experiments with Prompt Templates

The simplest way to get started is by using prompt templates:

```python
from galileo.datasets import get_dataset
from galileo.experiments import run_experiment
from galileo.prompts import get_prompt_template, create_prompt_template
from galileo.resources.models import Message, MessageRole

# Try to get an existing prompt template
prompt = get_prompt_template(
    name="storyteller-prompt",
    project="my-project"
)

# If the prompt doesn't exist, create it
if prompt is None:
    prompt = create_prompt_template(
        name="storyteller-prompt",
        project="my-project",
        messages=[
            Message(role=MessageRole.SYSTEM, content="You are a great storyteller."),
            Message(role=MessageRole.USER, content="Write a story about {input}")
        ]
    )

# Run an experiment with the prompt template
results = run_experiment(
    "story-experiment",
    dataset=get_dataset(name="storyteller-dataset"),
    prompt=prompt,
    metrics=["correctness"],
    project="my-project",
    tags=[]
)
```

## Running Experiments with Custom Functions

For more complex scenarios, you can use custom functions with the OpenAI wrapper:

```python
from galileo.datasets import get_dataset
from galileo.experiments import run_experiment
from galileo import log, openai

@log
def llm_call(data):
    return openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a great storyteller."},
            {"role": "user", content": f"Write a story about {data.input}"}
        ],
    ).choices[0].message.content

dataset = get_dataset(name="storyteller-dataset")

results = run_experiment(
    "story-function-experiment",
    dataset=dataset,
    function=llm_call,
    metrics=["correctness"],
    concurrency=1,  # number of rows to execute in parallel
    project="my-project",
    tags=[]
)
```

## Custom Dataset Evaluation

When you need to test specific scenarios:

```python
from galileo.experiments import run_experiment
from galileo import log, openai

@log
def llm_call(data):
    return openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a geography expert"},
            {"role": "user", "content": f"Which continent does the following country belong to: {data.input}"}
        ],
    ).choices[0].message.content

results = run_experiment(
    "geography-experiment",
    dataset=lambda: [
        {
            "input": "Spain",
            "expected": "Europe",
        },
    ],
    function=llm_call,
    metrics=["correctness"],
    concurrency=1,  # number of rows to execute in parallel
    project="my-project",
    tags=[]
)
```

## Custom Metrics for Deep Analysis

For sophisticated evaluation needs:

```python
from galileo.datasets import get_dataset
from galileo.experiments import run_experiment
from galileo import log, openai

@log
def llm_call(data):
    return openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a great storyteller."},
            {"role": "user", "content": f"Write a story about '{data.input}' and make it sound like a human wrote it."}
        ],
    ).choices[0].message.content

def check_for_delve(input, output, expected) -> int:
    return 1 if "delve" not in input else 0

dataset = get_dataset(name="storyteller-dataset")

results = run_experiment(
    "custom-metric-experiment",
    dataset=dataset,
    function=llm_call,
    metrics=[check_for_delve],
    concurrency=2,
    project="my-project",
    tags=[]
)
```

## API Reference

### run_experiment

```python
def run_experiment(
    name: str,
    dataset: Union[Dataset, List[Dict], Callable[[], List[Dict]]],
    prompt: Optional[PromptTemplate] = None,
    function: Optional[Callable] = None,
    metrics: List[Union[str, Callable[[Any, Any, Any], int]]] = None,
    project: str = None,
    concurrency: int = 1,
    tags: List[str] = None
) -> str:
```

Runs an experiment.

#### Parameters

- `name`: The name of the experiment
- `dataset`: Either a Dataset object, a list of dictionaries, or a callable that returns a list of dictionaries
- `prompt`: (Optional) A PromptTemplate to use for the experiment
- `function`: (Optional) A function that takes input data and returns a result
- `metrics`: List of metric names or custom metric functions
- `project`: The project to run the experiment in
- `concurrency`: Number of rows to execute in parallel
- `tags`: List of tags to associate with the experiment

#### Returns

A URL where you can view the experiment results.

### create_prompt_template

```python
def create_prompt_template(
    name: str,
    project: str,
    messages: List[Message]
) -> PromptTemplate:
```

Creates a new prompt template.

#### Parameters

- `name`: The name of the template
- `project`: The project to create the template in
- `messages`: List of Message objects defining the template

#### Returns

The created PromptTemplate object.

## Best Practices

1. **Use consistent datasets**: Use the same dataset when comparing different prompts or models to ensure fair comparisons.

2. **Test multiple variations**: Run experiments with different prompt variations to find the best approach.

3. **Use appropriate metrics**: Choose metrics that are relevant to your specific use case.

4. **Start small**: Begin with a small dataset to quickly iterate and refine your approach before scaling up.

5. **Document your experiments**: Keep track of what you're testing and why to make it easier to interpret results.

## Related Resources

- [Datasets](./experimentation/datasets) - Creating and managing datasets for experiments
- [Prompts](./prompts) - Creating and using prompt templates