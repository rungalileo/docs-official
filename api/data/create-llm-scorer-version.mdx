---
title: Create LLM Scorer Version
openapi: post /scorers/{scorer_id}/version/llm
---

LLM scorers leverage language models to evaluate and score AI outputs based on custom criteria. This endpoint allows you to create a new version of an LLM scorer by providing instructions and a template for how the evaluation should be performed. LLM scorers are particularly useful for subjective evaluations like tone, creativity, or adherence to complex requirements that are difficult to measure programmatically.

## Notes

- The instructions should clearly define the evaluation criteria
- Few-shot examples can significantly improve the consistency and quality of evaluations
- Each upload creates a new version of the scorer
- Previous versions remain accessible but the latest version will be used by default
