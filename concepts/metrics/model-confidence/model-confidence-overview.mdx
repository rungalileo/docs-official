---
title: Model Confidence Metrics 
description: Understand your AI's certainty in its responses with Galileo's model confidence metrics.
---

# Model Confidence Metrics Overview

Model confidence metrics help you gauge how certain your AI is about its answers. These metrics are useful for flagging uncertain responses, improving reliability, and knowing when to involve a human in the loop.

Use these metrics when you want to:
- Identify responses where the model is unsure or likely to make mistakes.
- Improve user trust by surfacing confidence scores or warnings.
- Analyze which prompts or situations are most challenging for your AI.

Our model confidence metrics include:

- **Uncertainty:** Measures the model's confidence in its generated response.  
  [Learn more →](uncertainty)
- **Prompt Perplexity:** Evaluates how difficult or unusual the prompt is for the model to process.  
  [Learn more →](prompt-perplexity)

Below is a quick reference table of all model confidence metrics:

<ModelConfidenceMetrics />

---

## Next Steps

- [Back to Metrics Overview](../overview)
- [Compare all metrics](../metric-comparison)
