---
title: Response Quality Metrics 
description: Evaluate the accuracy, completeness, and relevance of your AI's responses using Galileo's response quality metrics.
---

# Response Quality Metrics Overview

Response quality metrics help you measure how well your AI system answers user questions, follows instructions, and provides useful information. These metrics are key for building reliable, helpful, and user-friendly AI applications.

Use these metrics when you want to:
- Ensure your AI's responses are factually correct and complete.
- Check that the model follows instructions and uses retrieved information effectively.
- Evaluate how well your system grounds answers in context or source material.

Our response quality metrics include:

- **Completeness:** Measures whether the response addresses all aspects of the user's query.  
  [Learn more →](./completeness.mdx)
- **Correctness:** Evaluates the factual accuracy of information provided in the response.  
  [Learn more →](./correctness.mdx)
- **Instruction Adherence:** Assesses whether the model followed the instructions in your prompt template.  
  [Learn more →](./instruction-adherence.mdx)
- **Ground Truth Adherence:** Measures how well the response aligns with established ground truth.  
  [Learn more →](./ground-truth-adherence.mdx)
- **Chunk Relevance:** Evaluates whether the retrieved chunks are relevant to the user's query.  
  [Learn more →](./chunk-relevance.mdx)
- **Chunk Attribution:** Assesses whether the response properly attributes information to source documents.  
  [Learn more →](./chunk-attribution.mdx)
- **Chunk Utilization:** Measures how effectively the model uses the retrieved chunks in its response.  
  [Learn more →](./chunk-utilization.mdx)

Below is a quick reference table of all response quality metrics:

<ResponseQuality />

---

## Next Steps

- [Back to Metrics Overview](../overview.mdx)
- [Compare all metrics](../metric-comparison.mdx)
