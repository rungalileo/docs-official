---
title: Metrics Overview
description: Explore Galileo's comprehensive metrics framework for evaluating and improving AI system performance across multiple dimensions.
---

import { MetricCategories } from "/snippets/components/Metric-categories.mdx";


<script>
  {`
  function highlightSection() {
    // Remove any existing highlights
    const prevHighlight = document.querySelector('.section-highlight');
    if (prevHighlight) {
      prevHighlight.classList.remove('section-highlight');
    }

    // If we have a hash, find and highlight the section
    if (window.location.hash) {
      const targetSection = document.querySelector(window.location.hash);
      if (targetSection) {
        targetSection.classList.add('section-highlight');
        // Ensure smooth scroll
        targetSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
      }
    }
  }

  // Run on page load and when hash changes
  if (typeof window !== 'undefined') {
    window.addEventListener('load', highlightSection);
    window.addEventListener('hashchange', highlightSection);
  }
`}
</script>

<style>
  {`
  .section-highlight {
    position: relative;
    background: #4a2ff911;
    padding: 24px;
    margin: -24px;
    border-radius: 8px;
    border-left: 4px solid #4a2ff9;
    animation: flashHighlight 2s ease-out;
  }

  @keyframes flashHighlight {
    0% { background: #4a2ff933; }
    100% { background: #4a2ff911; }
  }
`}
</style>

Galileo provides a robust set of metrics to evaluate and improve your AI systems across multiple dimensions. These metrics help you identify issues, understand performance patterns, and implement targeted improvements to enhance your AI applications.

To calculate metrics, you will need to configure an integration with an LLM. Visit the relevant API platform to obtain an API key, then add it using the [integrations page](https://app.galileo.ai/settings/integrations) from within the Galileo console.

## Out-of-the-Box Metric Categories

Our metrics are organized into five key categories, each addressing a specific aspect of AI system performance:

  <MetricCategories />

## Response Quality Metrics

These metrics help you understand how well your AI system is responding to user queries:

<CardGroup cols={2}>
  <Card title="Completeness" href="/concepts/metrics/response-quality/completeness">
    Measures whether the response addresses all aspects of the user's query.

</Card>
  <Card title="Correctness" href="/concepts/metrics/response-quality/correctness">
    Evaluates the factual accuracy of information provided in the response.

</Card>
  <Card title="Instruction Adherence" href="/concepts/metrics/response-quality/instruction-adherence">
    Assesses whether the model followed the instructions in your prompt template.

</Card>
  <Card title="Ground Truth Adherence" href="/concepts/metrics/response-quality/ground-truth-adherence">
    Measures how well the response aligns with established ground truth.

</Card>
  <Card title="Chunk Relevance" href="/concepts/metrics/response-quality/chunk-relevance">
    Evaluates whether the retrieved chunks are relevant to the user's query.

</Card>
  <Card title="Chunk Attribution" href="/concepts/metrics/response-quality/chunk-attribution">
    Assesses whether the response properly attributes information to source documents.

</Card>
  <Card title="Chunk Utilization" href="/concepts/metrics/response-quality/chunk-utilization">
    Measures how effectively the model uses the retrieved chunks in its response.

</Card>
</CardGroup>

## Safety and Compliance Metrics

These metrics help identify potential risks and compliance issues:

<CardGroup cols={2}>
  <Card title="PII Detection" href="/concepts/metrics/safety-and-compliance/pii">
    Identifies personally identifiable information in prompts and responses.

</Card>
  <Card title="Prompt Injection" href="/concepts/metrics/safety-and-compliance/prompt-injection">
    Detects attempts to manipulate the model through malicious prompts.

</Card>
  <Card title="Toxicity" href="/concepts/metrics/safety-and-compliance/toxicity">
    Identifies harmful, offensive, or inappropriate content.

</Card>
  <Card title="Sexism" href="/concepts/metrics/safety-and-compliance/sexism">
    Detects gender-based bias or discriminatory content.

</Card>
</CardGroup>

## Model Confidence Metrics

These metrics help you understand the model's certainty in its responses:

<CardGroup cols={2}>
  <Card title="Uncertainty" href="/concepts/metrics/model-confidence/uncertainty">
    Measures the model's confidence in its generated response.

</Card>
  <Card title="Prompt Perplexity" href="/concepts/metrics/model-confidence/prompt-perplexity">
    Evaluates how difficult or unusual the prompt is for the model to process.

</Card>
</CardGroup>

## Agentic Performance Metrics

These metrics are specifically designed for AI agents that use tools:

<CardGroup cols={2}>
  <Card title="Tool Error" href="/concepts/metrics/agentic/tool-error">
    Detects errors or failures during the execution of tools.

</Card>
  <Card title="Tool Selection Quality" href="/concepts/metrics/agentic/tool-selection-quality">
    Evaluates whether the agent selected the most appropriate tools for the task.

</Card>
  <Card title="Action Advancement" href="/concepts/metrics/agentic/action-advancement">
    Measures how effectively each action advances toward the goal.

</Card>
</CardGroup>

## Expression and Readability Metrics

These metrics assess the linguistic quality of AI-generated content:

<CardGroup cols={2}>
  <Card title="Tone" href="/concepts/metrics/expression-and-readability/tone">
    Evaluates the emotional tone and style of the response.

</Card>
  <Card title="BLEU & ROUGE" href="/concepts/metrics/expression-and-readability/bleu-and-rouge">
    Standard NLP metrics for evaluating text generation quality.

</Card>
</CardGroup>

## Using Metrics Effectively

To get the most value from Galileo's metrics:

1. **Start with key metrics** - Focus on metrics most relevant to your use case
2. **Establish baselines** - Understand your current performance before making changes
3. **Track trends over time** - Monitor how metrics change as you iterate on your system
4. **Combine multiple metrics** - Look at related metrics together for a more complete picture
5. **Set thresholds** - Define acceptable ranges for critical metrics

## Next Steps

<CardGroup cols={2}>
  <Card title="Explore Response Quality" href="/concepts/metrics/response-quality/instruction-adherence">
    Dive deeper into metrics for evaluating response quality.

</Card>
</CardGroup>
