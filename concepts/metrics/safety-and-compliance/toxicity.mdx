---
title: Toxicity
description: Detect and prevent toxic content in AI systems using Galileo's Toxicity Metric to identify and mitigate harmful responses.
---

import { Scale } from '/snippets/components/scale.mdx';
import { DefinitionCard } from '/snippets/components/definition-card.mdx';

<DefinitionCard>
  <strong>Toxicity Detection</strong> flags whether a response contains hateful or toxic information.
</DefinitionCard>

### Categories of Toxicity

<Card>
  <div style={{display: 'flex', alignItems: 'center', gap: '0.5rem', marginBottom: '0.75rem'}}>
    <div style={{fontSize: '1.25rem', color: 'var(--primary-color)'}}>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
        <path d="M12 22c5.523 0 10-4.477 10-10S17.523 2 12 2 2 6.477 2 12s4.477 10 10 10z"></path>
        <path d="m9 12 2 2 4-4"></path>
      </svg>
    </div>
    <h3 style={{margin: 0, fontSize: '1.25rem', fontWeight: '600'}}>Types of Toxic Content</h3>
  </div>
  
  <div style={{marginTop: '1rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Hate Speech:</strong> Statements that demean, dehumanize, or attack individuals or groups based on identity factors like race, gender, or religion.
  </div>
  
  <div style={{marginTop: '0.75rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Offensive Content:</strong> Vulgar, abusive, or overly profane language used to provoke or insult.
  </div>
  
  <div style={{marginTop: '0.75rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Sexual Content:</strong> Explicit or inappropriate sexual statements that may be offensive or unsuitable in context.
  </div>
  
  <div style={{marginTop: '0.75rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Violence or Harm:</strong> Advocacy or description of physical harm, abuse, or violent actions.
  </div>
  
  <div style={{marginTop: '0.75rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Illegal or Unethical Guidance:</strong> Instructions or encouragement for illegal or unethical actions.
  </div>
  
  <div style={{marginTop: '0.75rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Manipulation or Exploitation:</strong> Language intended to deceive, exploit, or manipulate individuals for harmful purposes.
  </div>
</Card>

### Calculation Method
Toxicity detection is computed through a specialized process:

<Steps>
  <Step title="We leverage a Small Language Model (SLM) trained on open-source and internal datasets." />
  <Step title="The model achieves 96% accuracy on validation sets from multiple datasets:" />
  <Step title="Datasets used for validation include:" />
</Steps>

<CardGroup cols={3}>
  <Card title="Toxic Comment Classification Challenge" icon="comments">
    Open-source dataset for toxic content detection
  </Card>
  <Card title="Jigsaw Unintended Bias" icon="shield-halved">
    Dataset focused on identifying biased toxic content
  </Card>
  <Card title="Jigsaw Multilingual" icon="language">
    Multi-language toxic content classification
  </Card>
</CardGroup>

## Optimizing Your AI System

<Card>
  <div style={{display: 'flex', alignItems: 'center', gap: '0.5rem', marginBottom: '0.75rem'}}>
    <div style={{fontSize: '1.25rem', color: 'var(--primary-color)'}}>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
        <path d="M12 20h9"></path><path d="M16.5 3.5a2.121 2.121 0 0 1 3 3L7 19l-4 1 1-4L16.5 3.5z"></path>
      </svg>
    </div>
    <h3 style={{margin: 0, fontSize: '1.25rem', fontWeight: '600'}}>Addressing Toxicity in Your System</h3>
  </div>
  
  When toxic content is detected in your system, consider these approaches:
      
  <div style={{marginTop: '1rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Implement guardrails:</strong> Flag responses before being served to prevent future occurrences.
  </div>
  
  <div style={{marginTop: '0.75rem', paddingTop: '0.75rem', borderTop: '1px solid rgba(209, 213, 219, 0.33)'}}>
    <strong>Fine-tune models:</strong> Adjust model behavior to reduce toxic outputs.
  </div>
</Card>

<Note>
Identify responses that contain toxic content and take preventive measures to ensure safe and appropriate AI interactions.
</Note>