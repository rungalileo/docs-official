---
title: FAQ
---

## üß† General & Core Concepts

### Q: What is Galileo?

**Galileo** is an end-to-end platform for evaluating and improving the performance of generative AI models. It provides tools to help teams deeply understand model behavior across tasks like text generation, summarization, classification, and more. Galileo integrates seamlessly into your LLM pipeline and supports evaluation with both human feedback and automated metrics.

[View the Galileo platform overview ‚Üí](/what-is-galileo)

### Q: How does Galileo help with generative AI evaluation?

Galileo enables teams to evaluate generative models through:

- **Automatic and custom metrics**: BLEU, ROUGE, BERTScore, toxicity, hallucination detection, etc.
- **Human feedback integration**: Including labeler workflows and UI.
- **Fine-grained error analysis**: Spot low-quality generations, detect model drift, and segment failure patterns.
- **Multi-model comparison**: Evaluate different model outputs side by side.

This helps identify where your model is strong or weak and speeds up iteration cycles.

[Learn more about Galileo Experiments ‚Üí](/concepts/experiments)

### Q: Why is evaluating generative AI models different from traditional ML evaluation?

Unlike traditional ML, where outputs are often single values or labels, generative AI outputs are open-ended and nuanced (e.g. full text). This creates challenges like:

- **Subjectivity**: There's often no single correct answer.
- **Evaluation complexity**: Metrics must handle variation, style, relevance, and factual accuracy.
- **Human oversight**: Required to understand intent and impact of generations.

Galileo addresses these differences with custom workflows tailored to generative AI.

[Learn more about Galileo‚Äôs evaluation approach ‚Üí](/concepts/metrics/overview)

### Q: How do I get started with Galileo to evaluate my AI models?

To get started:

1. **Sign up** for a Galileo account.  
2. **Instrument your model pipeline** using Galileo‚Äôs SDK or integrate with the platform‚Äôs Python APIs.  
3. **Log prompts and generations** to the Galileo Console.  
4. **Run evaluations** using built-in or custom metrics.  
5. **Analyze and iterate** with visualizations and error slices.

[Getting started guide ‚Üí](/getting-started/quickstart)

### Q: How does Galileo fit into the generative AI development lifecycle?

Galileo supports every stage of generative AI development:

- **Prompt & data experimentation**: Evaluate prompt effectiveness.
- **Model comparison**: Choose the best model or fine-tuned variant.
- **Fine-tuning oversight**: Identify when training introduces regressions.
- **Pre-deployment checks**: Surface edge cases, hallucinations, or bias.
- **Post-deployment monitoring**: Continuously analyze performance and drift.

It becomes your team's co-pilot in delivering high-quality AI products.

[See Galileo in the AI workflow ‚Üí](/how-to-guides/overview)

---

## üñ•Ô∏è SDK Usage FAQ

### Q: What SDKs does Galileo provide?

Galileo provides Python and Typescript SDK designed to integrate with your generative AI pipelines. They allow you to log prompts, model generations, evaluation metrics, and feedback directly from your application.

You can install the SDK via the command line:

<CodeGroup>
```bash Python
pip install galileo
```

```bash Typescript
npm install galileo
```
</CodeGroup>

[Python SDK installation instructions ‚Üí](/sdk-api/python/overview)
[Typescript SDK installation instructions ‚Üí](/sdk-api/typescript/overview)

### Q: How do I log a prompt and response using the SDK?

You can log a prompt and generation using the `log_sample()` method:

```python
from galileo import GalileoLogger

logger = GalileoLogger(task_type="text-generation")

logger.log_sample(
    prompt="Summarize the following article...",
    generation="The article discusses...",
    metadata={"source": "news_article_123"}
)
```

This data will then be visible in the Galileo Console for further evaluation.

[More on logging data ‚Üí](/getting-started/logging)

### Q: Can I log custom metrics with the SDK?

Yes, Galileo allows you to log both built-in and custom metrics. You can use the `log_metric()` method after evaluating a generation:

```python
logger.log_metric(
    sample_id="sample_001",
    metric_name="custom_readability_score",
    value=0.92
)
```

Custom metrics help tailor evaluation to your specific use case.

[Custom metric logging guide ‚Üí](/concepts/metrics/overview)



### Q: How do I batch log multiple samples?

To batch log data, use the `log_samples()` method with a list of dictionaries:

```python
samples = [
    {
        "prompt": "Write a poem about the ocean.",
        "generation": "The waves crash soft and deep..."
    },
    {
        "prompt": "Explain black holes.",
        "generation": "Black holes are regions in space..."
    }
]

logger.log_samples(samples)
```

Batch logging is useful for processing datasets or logging results from async tasks.

[Batch logging documentation ‚Üí](/getting-started/logging)

### Q: How can I tag or categorize different experiments or datasets in my logs?

Use the `metadata` field to attach tags, experiment IDs, or dataset names. This allows you to slice and compare data later in the Galileo UI.

```python
logger.log_sample(
    prompt="Summarize the following email...",
    generation="Here's a summary...",
    metadata={"dataset": "customer_support", "experiment_id": "v2_ablation"}
)
```

These tags make it easier to drill into specific subsets of data during evaluation.

[Working with metadata ‚Üí](/concepts/spans)

### Q: Can I log streaming outputs or partial generations?

Yes ‚Äî Galileo supports streaming workflows by allowing you to log samples incrementally. While you can log the final generation normally, you can also update or re-log a sample with the same `sample_id` if you‚Äôre assembling the response over time.

```python
logger.log_sample(
    sample_id="stream_001",
    prompt="Write a poem line by line.",
    generation="The sun rose slow across the bay..."
)
```

Make sure to use a consistent `sample_id` if you plan to overwrite or update logged data.

[Working with metadata ‚Üí](/concepts/spans)

### Q: How do I handle logging sensitive data like PII?

If your prompts or generations may contain personally identifiable information (PII), ensure that you mask, anonymize, or exclude that data before logging. Galileo recommends using metadata fields to mark sensitive content without logging it directly.

```python
logger.log_sample(
    prompt="[REDACTED]",
    generation="[REDACTED]",
    metadata={"pii_redacted": True}
)
```

You can also add preprocessing steps in your pipeline to sanitize the data automatically.

[Security best practices ‚Üí](/concepts/metrics/safety-and-compliance/pii)

### Q: What‚Äôs the best way to log generations during async or batch inference jobs?

If you‚Äôre running asynchronous model inference or processing batches of generations in a job queue, you can use the `log_samples()` method to send logs in groups. This is both efficient and compatible with job processing pipelines.

```python
results = run_model_async(prompts)

samples = [
    {"prompt": r["input"], "generation": r["output"], "metadata": {"batch": "evening_test_run"}}
    for r in results
]

logger.log_samples(samples)
```

This approach is ideal for logging from cron jobs, workers, or serverless functions.

[Using variables for batch logging ‚Üí](/concepts/playground#benefits-of-using-variables)

---

## üåê API Integration FAQs

### Q: How do I authenticate with the Galileo API?

You must include an `Authorization` header with a valid bearer token in every request:

```http
Authorization: Bearer YOUR_API_TOKEN
```

You can find or generate your token in the Galileo Console under your account settings.

[Get your API key ‚Üí](https://app.galileo.ai/settings/api-keys)

### Q: How do I log a sample via the API?

Send a `POST` request to the `/samples` endpoint with a JSON payload:

```json
{
  "task_type": "text-generation",
  "prompt": "Summarize this...",
  "generation": "This is a summary...",
  "metadata": {
    "source": "email_batch_1"
  }
}
```

You must include all required fields manually, including `task_type` and structured metadata.

[Logging via API ‚Üí](/getting-started/logging)

### Q: What‚Äôs the best way to handle errors and retries with the API?

Unlike the SDK, the API does not automatically retry failed requests. You‚Äôll need to:

- Check for `4XX` and `5XX` status codes
- Parse error messages in the response body
- Implement retry logic using exponential backoff

This is especially important when logging data from production pipelines.

[Error handling reference ‚Üí](/references/faqs/errors)

### Q: Can I update a previously logged sample using the API?

Yes, you can update a sample by including the `sample_id` you used when initially logging. If you send a new sample with the same `sample_id`, Galileo will overwrite it.

This is useful for streaming or progressive generation workflows.

```json
{
  "sample_id": "stream_001",
  "prompt": "Write a haiku",
  "generation": "Clouds drift slowly by"
}
```

[Sample ID example ‚Üí](/concepts/spans)

### Q: How do I include evaluation metrics when logging via the API?

To log evaluation metrics along with your samples, you must send a separate `POST` request to the `/metrics` endpoint. Include the corresponding `sample_id`, `metric_name`, and `value`:

```json
{
  "sample_id": "sample_001",
  "metric_name": "bleu_score",
  "value": 0.87
}
```

You can log multiple metrics for the same sample by sending multiple requests or using a batch format if supported.

[Metric logging ‚Üí](/concepts/metrics/overview)


### Q: Can I retrieve data I‚Äôve previously logged to Galileo via the API?

Yes, the API includes endpoints for retrieving samples, metrics, and project data. For example, to fetch logged samples:

```http
GET /samples?project_id=your_project_id
```

You can filter results by metadata, task type, or time range to analyze data or sync with downstream tools.

[Data retrieval and chunk attribution ‚Üí](/concepts/metrics/response-quality/chunk-attribution)


### Q: How do I structure metadata for complex use cases (e.g. nested values)?

Metadata must be a flat key-value object. Nested or deeply structured metadata (e.g. JSON objects within JSON) should be flattened:

```json
"metadata": {
  "dataset": "test_a",
  "run_type": "ablation",
  "group": "control"
}
```

Avoid nesting values within arrays or objects, as these may not render properly in the Galileo UI.

[Metadata formatting tips ‚Üí](/concepts/spans)


### How can I test the Galileo API without writing a full app integration?

You can use tools like [Postman](https://www.postman.com/) or [cURL](https://curl.se/) to send test requests manually. Here's an example using cURL:

```bash
curl -X POST https://api.galileo.ai/samples \
  -H "Authorization: Bearer YOUR_API_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"task_type": "text-generation", "prompt": "Hello", "generation": "Hi there!"}'
```

This is helpful for debugging or trying out new endpoints quickly.

---

## üí≥ Pricing and Billing FAQ

### Q: How is pricing structured for Galileo?

Galileo offers a usage-based pricing model that scales with your team. Plans typically vary based on number of evaluations, volume of logged data, and access to advanced features like human feedback workflows or custom metrics. For enterprise deployments, pricing is customized based on team size and requirements.

[View pricing details ‚Üí](https://www.galileo.ai/pricing)

### Q: Is there a free trial or free tier available?

Yes, Galileo offers a free tier with limited usage to help you evaluate the platform. You can log a limited number of samples and run basic evaluations. This is ideal for small teams or individual developers starting out.

[Sign up to start free ‚Üí](https://www.galileo.ai/signup)

### Q: How do I upgrade my account or change my billing plan?

You can upgrade, downgrade, or change your billing plan directly from the Galileo Console under the **Billing** section in your account settings. If you're on a custom enterprise plan, reach out to your Galileo representative to discuss changes.

[Manage billing ‚Üí](https://app.galileo.ai)

### Q: Can I track usage and billing details in real time?

Yes, the Galileo Console provides visibility into your usage metrics, evaluation volume, and billing activity. You can view your current plan limits and projected charges at any time.

[Open a project to view usage data ‚Üí](https://app.galileo.ai)

### Q: What payment methods are accepted?

Galileo accepts major credit cards for self-service accounts. For enterprise customers, additional options like invoicing, ACH, and purchase orders may be available depending on the agreement.

[Billing and payment info ‚Üí](https://www.galileo.ai/pricing)

---


## üîå Integration and Compatibility

### Q: Which LLM providers and frameworks does Galileo support out of the box?

Galileo supports leading LLM providers including OpenAI, Azure OpenAI, Anthropic, Cohere, and open-source models like LLaMA, Mistral, and Falcon. It integrates well with frameworks such as LangChain, LlamaIndex, and custom Python pipelines. Galileo‚Äôs flexible SDK and API allow it to work with any provider as long as you can log prompts and generations.

[View supported integrations in Galileo Console ‚Üí](https://app.galileo.ai/)

### Q: Does Galileo integrate with LlamaIndex or other retrieval frameworks for RAG pipelines?

Yes, Galileo works with LlamaIndex (formerly GPT Index) and other retrieval-augmented generation frameworks. You can log RAG context, query, and generation data as part of a single sample or as separate, chained samples. This makes it easier to analyze the quality of both retrieved context and final generations.

[RAG pipeline logging ‚Üí](/how-to-guides/rag/basic-example)

### Q: Does Galileo support multilingual evaluation?

Yes, Galileo can be used to evaluate generations in any language. Built-in metrics like BLEU or ROUGE work best when reference outputs are provided in the same language. You can also log custom metrics specific to a language or region to reflect accuracy or appropriateness.

[BLEU and ROUGE ‚Üí](/concepts/metrics/expression-and-readability/bleu-and-rouge)


---

## üõ†Ô∏è Troubleshooting & Best Practices

### Q: What should I do if Galileo is not logging any data from my application?

First, confirm that your API key or SDK credentials are correctly configured. Next, check that your network or firewall settings are not blocking outbound requests to Galileo. Review your logs to ensure the logging functions are being called, and verify that the data format matches Galileo's requirements. If you're using the SDK, make sure the `log_sample()` or `log_samples()` calls are executed within the code path you're testing.

[Troubleshooting logging issues ‚Üí](/references/faqs/errors)

### Q: How do I troubleshoot unexpected evaluation results or scores in Galileo?

Start by reviewing the evaluation configuration, including which metrics are being applied and how they are calculated. If you‚Äôre using custom metrics, verify that they are computed correctly and that the input and reference values are accurate. Use Galileo‚Äôs error slices and segmentation tools to identify patterns in where scores drop. It‚Äôs also helpful to manually inspect low-scoring examples to rule out model or data issues.

[Evaluating Metrics ‚Üí](/concepts/metrics/response-quality/instruction-adherence)

### Q: What are best practices for creating an evaluation dataset or test set for Galileo?

Use representative prompts and outputs that reflect your production use cases. Include both typical and edge-case examples to ensure robustness. Tag samples with metadata like version, domain, or scenario to enable slicing. Avoid leaking answers or clues into the prompts, and ensure that the ground truth or expectations are well-defined if you're using metrics that rely on references.

```python
from galileo import GalileoLogger
logger = GalileoLogger(task_type="text-generation")
evaluation_set = [
    {
        "prompt": "Explain the difference between TCP and UDP.",
        "generation": "TCP is connection-based, UDP is connectionless...",
        "metadata": {
            "topic": "networking",
            "difficulty": "medium",
            "scenario": "interview_prep",
            "split": "eval_v1"
        }
    },
    {
        "prompt": "Translate 'How are you?' to Japanese.",
        "generation": "„ÅäÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü",
        "metadata": {
            "task": "translation",
            "language": "ja",
            "split": "eval_v1"
        }
    }
]
```

[Dataset design tips ‚Üí](/concepts/traces)

### Q: How do I handle errors or timeouts when evaluating long prompts or outputs?

Long inputs or generations can occasionally hit system or model limits. Break up extremely long inputs into chunks when possible, and monitor token counts if you're using hosted LLMs with token caps. If evaluating through the SDK or API, add retry logic and ensure that the timeout settings are reasonable. Galileo also supports logging partial generations with the same `sample_id` to update them later.

```python
long_input = " ".join(["This is a sentence."] * 1000)  # ~20K tokens

# Break into chunks
chunk_size = 500  # characters or tokens, depending on your tokenizer
chunks = [long_input[i:i+chunk_size] for i in range(0, len(long_input), chunk_size)]

for i, chunk in enumerate(chunks):
    generated_text = f"Generated response for chunk {i + 1}"
    logger.log_sample(
        sample_id=sample_id,
        prompt=chunk,
        generation=generated_text,
        metadata={"chunk_index": i + 1, "is_chunked": True}
    )
```


### How can I ensure my evaluation metrics align with real user needs or product goals?

Start by understanding the key outcomes your product is driving‚Äîaccuracy, clarity, helpfulness, safety, etc. Choose evaluation metrics that reflect those priorities. Galileo supports custom metric logging, so you can track user satisfaction scores, pass/fail labels, or scenario-specific metrics alongside standard ones. Regularly review evaluation slices with your team to validate alignment with business outcomes.

[Metrics overview ‚Üí](/concepts/metrics/overview)

---

If issues persist, contact support at [support@rungalileo.io](mailto:support@rungalileo.io) with your request payload and full error message.