export const ModelConfidenceMetrics = () => (
  <div>
    <table style={{ width: "100%", borderCollapse: "collapse", textAlign: "left" }}>
      <thead>
        <tr>
          <th style={{ textAlign: "left", paddingBottom: "8px", paddingRight: "24px" }}>Name</th>
          <th style={{ textAlign: "left", paddingBottom: "8px" }}>Description</th>
          <th style={{ textAlign: "left", paddingBottom: "8px" }}>When to Use</th>
          <th style={{ textAlign: "left", paddingBottom: "8px" }}>Example Use Case</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style={{ paddingRight: "24px" }}>
            <a href="/concepts/metrics/model-confidence/uncertainty">Uncertainty</a>
          </td>
          <td>Measures the model's confidence in its generated response.</td>
          <td>When you want to understand how certain the model is about its answers.</td>
          <td>Flagging responses where the model is unsure, so a human can review them before sending to a user.</td>
        </tr>
        <tr>
          <td style={{ paddingRight: "24px" }}>
            <a href="/concepts/metrics/model-confidence/prompt-perplexity">Prompt Perplexity</a>
          </td>
          <td>Evaluates how difficult or unusual the prompt is for the model to process.</td>
          <td>When you want to identify prompts that may confuse the model or lead to lower-quality responses.</td>
          <td>Detecting outlier prompts in a customer support chatbot to improve prompt engineering.</td>
        </tr>
      </tbody>
    </table>
  </div>
); 