import React from 'react';
export const MetricComparisonTable = () => (
  <div>
    <table style={{ width: "100%", borderCollapse: "collapse", textAlign: "left" }}>
      <thead>
        <tr>
          <th style={{ textAlign: "left", paddingBottom: "8px" }}>Name</th>
          <th style={{ textAlign: "left", paddingBottom: "8px" }}>Category</th>
          <th style={{ textAlign: "left", paddingBottom: "8px" }}>Description</th>
          <th style={{ textAlign: "left", paddingBottom: "8px" }}>When to Use</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><a href="/concepts/metrics/agentic/action-advancement">Action Advancement → </a></td>
          <td>Agentic</td>
          <td>Measures how effectively each action advances toward the goal.</td>
          <td>When assessing whether an agent is making meaningful progress in multi-step tasks.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/agentic/action-completion">Action Completion → </a></td>
          <td>Agentic</td>
          <td>Measures whether the agent completed the intended action.</td>
          <td>When evaluating agent task completion rates and success.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/chunk-attribution">Chunk Attribution → </a></td>
          <td>Response Quality</td>
          <td>Assesses whether the response properly attributes information to source documents.</td>
          <td>When implementing RAG systems and want to ensure proper attribution.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/chunk-utilization">Chunk Utilization → </a></td>
          <td>Response Quality</td>
          <td>Measures how effectively the model uses the retrieved chunks in its response.</td>
          <td>When optimizing RAG performance to ensure retrieved information is used efficiently.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/completeness">Completeness → </a></td>
          <td>Response Quality</td>
          <td>Measures whether the response addresses all aspects of the user's query.</td>
          <td>When evaluating if responses fully address the user's intent.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/context-adherence">Context Adherence → </a></td>
          <td>Response Quality</td>
          <td>Measures how well the response aligns with the provided context.</td>
          <td>When you want to ensure the model is grounding its responses in the provided context.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/context-relevance">Context Relevance (Query Adherence) → </a></td>
          <td>Response Quality</td>
          <td>Evaluates whether the retrieved context is relevant to the user's query.</td>
          <td>When assessing the quality of your retrieval system's results.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/correctness">Correctness (factuality) → </a></td>
          <td>Response Quality</td>
          <td>Evaluates the factual accuracy of information provided in the response.</td>
          <td>When accuracy of information is critical to your application.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/ground-truth-adherence">Ground Truth Adherence → </a></td>
          <td>Response Quality</td>
          <td>Measures how well the response aligns with established ground truth.</td>
          <td>When evaluating model responses against known correct answers.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/response-quality/instruction-adherence">Instruction Adherence → </a></td>
          <td>Response Quality</td>
          <td>Assesses whether the model followed the instructions in your prompt template.</td>
          <td>When using complex prompts and need to verify the model is following all instructions.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/safety-and-compliance/pii">PII / CPNI / PHI → </a></td>
          <td>Safety and Compliance</td>
          <td>Identifies personally identifiable or sensitive information in prompts and responses.</td>
          <td>When handling potentially sensitive data or in regulated industries.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/safety-and-compliance/prompt-injection">Prompt Injection → </a></td>
          <td>Safety and Compliance</td>
          <td>Detects attempts to manipulate the model through malicious prompts.</td>
          <td>When allowing user input to be processed directly by your AI system.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/safety-and-compliance/sexism">Sexism / Bias → </a></td>
          <td>Safety and Compliance</td>
          <td>Detects gender-based bias or discriminatory content.</td>
          <td>When ensuring AI outputs are free from bias and discrimination.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/agentic/tool-error">Tool Errors → </a></td>
          <td>Agentic</td>
          <td>Detects errors or failures during the execution of tools.</td>
          <td>When implementing AI agents that use tools and want to track error rates.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/agentic/tool-selection-quality">Tool Selection Quality → </a></td>
          <td>Agentic</td>
          <td>Evaluates whether the agent selected the most appropriate tools for the task.</td>
          <td>When optimizing agent systems for effective tool usage.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/safety-and-compliance/toxicity">Toxicity → </a></td>
          <td>Safety and Compliance</td>
          <td>Identifies harmful, offensive, or inappropriate content.</td>
          <td>When monitoring AI outputs for harmful content or implementing content filtering.</td>
        </tr>
        <tr>
          <td><a href="/concepts/metrics/expression-and-readability/tone">Tone → </a></td>
          <td>Expression and Readability</td>
          <td>Evaluates the emotional tone and style of the response.</td>
          <td>When the style and tone of AI responses matter for your brand or user experience.</td>
        </tr>
      </tbody>
    </table>
  </div>
);