```python Python
from galileo import galileo_context
from galileo.openai import openai

def openai_call():
    response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "What is the capital of France?"}]
        )

    return response.choices[0].message.content

# This will create a trace and automatically add spans for OpenAI calls
with galileo_context(project="my-project", log_stream="my-log-stream"):
    # This will create an llm span
    result = openai_call()
    print(result)
```
