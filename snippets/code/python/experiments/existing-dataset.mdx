```python Python
from galileo.experiments import run_experiment
from galileo.datasets import get_dataset
from galileo import log, openai

@log
def llm_call(data):
	return openai.chat.completions.create(
        model="gpt-4",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about {data.input}"}
        ],
    ).choices[0].message.content

dataset = get_dataset(name="storyteller-dataset")

results = run_experiment(
	"story-function-experiment",
	dataset=dataset,
	function=llm_call,
	metrics=["correctness"],
	concurrency=1, # number of rows to execute in parallel,
	project="my-project",
	tags=[]
) 
```