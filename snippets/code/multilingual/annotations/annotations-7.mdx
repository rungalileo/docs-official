```bash Python
import os
from galileo import galileo_context, GalileoLogger # The Galileo context manager
from galileo.openai import openai # The Galileo OpenAI client wrapper
from dotenv import load_dotenv
load_dotenv()

# Initialize the OpenAI client using the Galileo wrapper and your OpenAI API key
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# The Galileo context manager will automatically create a Trace and flush the logs when exiting
with galileo_context():
    prompt = f"Explain the following topic succinctly: Newton's First Law"
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": prompt}],
    )
    print(response.choices[0].message.content.strip())

    answer = response.choices[0].message.content.strip()
    
    ## Define tags and metadata
    tags = ["newton", "test", "new-version"]
    metadata = {"experimentNumber": "1", \
                "promptVersion": "0.0.1", \
                "field": "physics"}

    ## Initialize logger and select project and log stream
    logger = GalileoLogger(project="Science Exploration", log_stream="laws-of-science")

    ## Initialize a new Trace and start listening for logs to add to it
    trace = logger.start_trace(
        input=prompt,
        tags=tags,
        metadata=metadata
    )

    ## Create a span to log LLM outputs. This is captured by the Trace
    logger.add_llm_span(
        input=[{"role": "system", "content": prompt}],
        output=response.choices[0].message.content,
        model="gpt-4o",
        tags=tags,
        metadata=metadata
    )

    ## Close the Trace and push captured logs to it
    logger.conclude(answer)
    logger.flush()
```

```bash Typescript
import * as dotenv from "dotenv";
dotenv.config();

import { withGalileo, GalileoLogger } from "galileo";
import { OpenAI } from "openai";
import { wrapOpenAI } from "galileo/wrappers";

const client = wrapOpenAI(new OpenAI({ apiKey: process.env.OPENAI_API_KEY ?? "" }));

await withGalileo(
  { project: "Science Exploration", logStream: "laws-of-science" },
  async () => {
    const prompt = `Explain the following topic succinctly: Newton's First Law`;

    const response = await client.chat.completions.create({
      model: "gpt-4o",
      messages: [{ role: "system", content: prompt }]
    });

    const answer: string = response.choices[0].message.content.trim();

    // Define tags and metadata
    const tags = ["newton", "test", "new-version"];
    const metadata = {
      experimentNumber: "1",
      promptVersion: "0.0.1",
      field: "physics"
    };

    // Initialize logger and select project and log stream
    const logger = new GalileoLogger({
      project: "Science Exploration",
      logStream: "laws-of-science"
    });

    // Initialize a new Trace and start listening for logs to add to it
    const trace = logger.startTrace({
      input: prompt,
      tags,
      metadata
    });

    // Create a span to log LLM outputs. This is captured by the Trace
    logger.addLlmSpan({
      input: [{ role: "system", content: prompt }],
      output: response.choices[0].message.content,
      model: "gpt-4o",
      tags,
      metadata
    });

    // Close the Trace and push captured logs to it
    logger.conclude(answer);
    logger.flush();
  }
);
```