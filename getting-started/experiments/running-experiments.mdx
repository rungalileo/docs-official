---
title: Running Experiments with Code
icon: "code"
syncTabs: true
---

import SnippetExperimentsPromptPython from "/snippets/code/python/experiments/prompt.mdx";
import SnippetExperimentsPromptTypescript from "/snippets/code/typescript/experiments/prompt.mdx";
import SnippetExperimentsExistingDatasetPython from "/snippets/code/python/experiments/existing-dataset.mdx";
import SnippetExperimentsExistingDatasetTypescript from "/snippets/code/typescript/experiments/existing-dataset.mdx";
import SnippetExperimentsCustomDatasetPython from "/snippets/code/python/experiments/custom-dataset.mdx";
import SnippetExperimentsCustomDatasetTypescript from "/snippets/code/typescript/experiments/custom-dataset.mdx";
import SnippetExperimentsCustomMetricsPython from "/snippets/code/python/experiments/custom-metrics.mdx";
import SnippetExperimentsCustomMetricsTypescript from "/snippets/code/typescript/experiments/custom-metrics.mdx";

As you progress from initial testing to systematic evaluation, you'll want to run experiments to validate your application's performance and behavior. Here are several ways to structure your experiments, starting from the simplest approaches and moving to more sophisticated implementations:

## Working with Prompts

The simplest way to get started with experimentation is by evaluating prompts directly against datasets. This is especially valuable during the initial prompt development and refinement phase, where you want to test different prompt variations. Assuming you've previously created a dataset, you can use the following code to run an experiment:

<CodeGroup>
  <SnippetExperimentsPromptPython />
  <SnippetExperimentsPromptTypescript />
</CodeGroup>

## Running Experiments with Custom Functions

Once you're comfortable with basic prompt testing, you might want to evaluate more complex parts of your app using your datasets. This approach is particularly useful when you have a generation function in your app that takes a set of inputs, which you can model with a dataset:

<Note>
This example uses OpenAI as the LLM being evaluated, and for generating metrics.

Galileo is model-agnostic, and supports leading LLM providers including OpenAI, Azure OpenAI, Anthropic, and LLaMA.
</Note>

<CodeGroup>
  <SnippetExperimentsExistingDatasetPython />
  <SnippetExperimentsExistingDatasetTypescript />
</CodeGroup>

## Custom Dataset Evaluation

As your testing needs become more specific, you might need to work with custom or local datasets. This approach is perfect for focused testing of edge cases or when building up your test suite with specific scenarios:

<CodeGroup>
  <SnippetExperimentsCustomDatasetPython />
  <SnippetExperimentsCustomDatasetTypescript />
</CodeGroup>

## Custom Metrics for Deep Analysis

For the most sophisticated level of testing, you might need to track specific aspects of your application's behavior. Custom metrics provide the flexibility to define precisely what you want to measure, enabling deep analysis and targeted improvement:

<CodeGroup>
  <SnippetExperimentsCustomMetricsPython />
  <SnippetExperimentsCustomMetricsTypescript />
</CodeGroup>

Each of these experimentation approaches fits into different stages of your development and testing workflow. As you progress from simple prompt testing to sophisticated custom metrics, Galileo's experimentation framework provides the tools you need to gather insights and improve your application's performance at every level of complexity.

## Experimenting with Agentic and RAG Applications

The experimentation framework extends naturally to more complex applications like agentic AI systems and RAG (Retrieval-Augmented Generation) applications. When working with agents, you can evaluate various aspects of their behavior, from decision-making capabilities to tool usage patterns. This is particularly valuable when testing how agents handle complex workflows, multi-step reasoning, or tool selection.

For RAG applications, experimentation helps validate both the retrieval and generation components of your system. You can assess the quality of retrieved context, measure response relevance, and ensure that your RAG pipeline maintains high accuracy across different types of queries. This is especially important when fine-tuning retrieval parameters or testing different reranking strategies.

The same experimentation patterns shown above apply to these more complex systems. You can use predefined datasets to benchmark performance, create custom datasets for specific edge cases, and define specialized metrics that capture the unique aspects of agent behavior or RAG performance. This systematic approach to testing helps ensure that your advanced AI applications maintain high quality and reliability in production environments.
