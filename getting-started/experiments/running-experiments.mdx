---
title: Running Experiments
icon: "lightbulb"
---

As you progress from initial testing to systematic evaluation, you'll want to run experiments to validate your application's performance and behavior. Here are several ways to structure your experiments, starting from the simplest approaches and moving to more sophisticated implementations:

## Working with Prompts

The simplest way to get started with experimentation is by evaluating prompts directly against datasets. This is especially valuable during the initial prompt development and refinement phase, where you want to test different prompt variations:

<CodeGroup>
```python run_experiment.py
from galileo.prompts import get_prompt
from galileo.experiments import run_experiment
from galileo.datasets import get_dataset
from galileo.scorers import correctness

prompt = get_prompt(
	name="storyteller-prompt"
	version="latest"
)

results = run_experiment(
	dataset=get_dataset(name="storyteller-dataset"),
	prompt,
	metrics=["correctness"],
	project="my-project",
	tags=[],
)
```

```typescript run_experiment.ts
// Coming soon...
```
</CodeGroup>

## Running Experiments with Existing Datasets

Once you're comfortable with basic prompt testing, you might want to evaluate more complex functions against pre-existing datasets. This approach is particularly useful when you've already collected a set of test cases or want to benchmark against known examples:

```python
from galileo.experiments import run_experiment
from galileo.datasets import get_dataset
#from galileo.metrics import correctness
from galileo import log, openai

@log
def llm_call(data):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about {data.input}"}
        ],
    ).choices[0].message.content

dataset = get_dataset(name="storyteller-dataset")

results = run_experiment(
	dataset,
	function=llm_call,
	metrics=["correctness"]
	concurrency=1, # number of rows to execute in parallel,
	project="my-project",
	tags=[],
)
```

## Custom Dataset Evaluation

As your testing needs become more specific, you might need to work with custom datasets. This approach is perfect for focused testing of edge cases or when building up your test suite with specific scenarios:

```python
from galileo.experiments import run_experiment
from galileo.scorers import correctness
from galileo import log, openai

@log
def llm_call(data):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a geography expert"},
          {"role": "user", "content": f"Which continent does the following country belong to: {data.input}"}
        ],
    ).choices[0].message.content

results = run_experiment(
	dataset=lambda: [
        {
            "input": "Spain",
            "expected": "Europe",
        },
    ],
	function=llm_call,
	metrics=["correctness"]
	concurrency=1, # number of rows to execute in parallel,
	project="my-project",
	tags=[],
)
```

## Custom Metrics for Deep Analysis

For the most sophisticated level of testing, you might need to track specific aspects of your application's behavior. Custom metrics provide the flexibility to define precisely what you want to measure, enabling deep analysis and targeted improvement:

```python
from galileo.experiments import run_experiment
from galileo.datasets import get_dataset
from galileo import log, openai

@log
def llm_call(data):
	return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": f"Write a story about '{data.input}' and make it sound like a human wrote it."}
        ],
    ).choices[0].message.content

def check_for_delve(input, output, expected) -> int:
	return 1 if "delve" not in input else 0

dataset = get_dataset(name="storyteller-dataset")

results = run_experiment(
	dataset,
	function=llm_call,
	metrics=[check_for_delve],
	concurrency=2,	
project="my-project",
	tags=[],
)
```

Each of these experimentation approaches fits into different stages of your development and testing workflow. As you progress from simple prompt testing to sophisticated custom metrics, Galileo's experimentation framework provides the tools you need to gather insights and improve your application's performance at every level of complexity.

## Experimenting with Agentic and RAG Applications

The experimentation framework extends naturally to more complex applications like agentic AI systems and RAG (Retrieval-Augmented Generation) applications. When working with agents, you can evaluate various aspects of their behavior, from decision-making capabilities to tool usage patterns. This is particularly valuable when testing how agents handle complex workflows, multi-step reasoning, or tool selection.

For RAG applications, experimentation helps validate both the retrieval and generation components of your system. You can assess the quality of retrieved context, measure response relevance, and ensure that your RAG pipeline maintains high accuracy across different types of queries. This is especially important when fine-tuning retrieval parameters or testing different reranking strategies.

The same experimentation patterns shown above apply to these more complex systems. You can use predefined datasets to benchmark performance, create custom datasets for specific edge cases, and define specialized metrics that capture the unique aspects of agent behavior or RAG performance. This systematic approach to testing helps ensure that your advanced AI applications maintain high quality and reliability in production environments.

