---
title: Running Experiments in the Galileo Console
icon: "hand-pointer"
---

This section will guide you through the process of running experiments in the Galileo Console.

## üö∂ Experiment Walkthrough

Follow these steps to test and improve your AI projects using [Galileo's Console UI](https://app.galileo.ai).

### Step 1: Select Project & Open Playground

On [app.galileo.ai](https://app.galileo.ai), use the **drop down menu in the top-left** to select the project you would like to experiment with. Or, create a new project.

Then, click the **"Open Playground"** button to access the Galileo Console.

![app.galileo.ai](/images/console-ui/home-navigation.png)

### Step 2: Select Model & Enter API Keys

In the Galileo Console UI, **select a model** using the "Model" drop down menu.

Some models require that you **enter your corresponding API key**. Visit their respective API platforms to obtain your keys.

![Select a Model](/images/console-ui/model-select.png)

### Step 3: Configure Model Settings

Click the settings icon **to the right of your model name** to adjust its behavior:

- **Max Length:** Sets the maximum number of tokens the model can generate in its output.
- **Temperature:** Controls randomness in output‚Äîhigher values make responses more creative, lower values make them more focused and deterministic.
- **Top P:** Limits sampling to the most likely tokens whose cumulative probability is within this threshold (a form of nucleus sampling).
- **Frequency Penalty:** Reduces the likelihood of the model repeating the same tokens by penalizing frequent ones.
- **Presence Penalty:** Discourages the model from mentioning tokens that have already appeared, promoting new content.

![Configure Model Settings](/images/console-ui/model-settings.png)

### Step 4: Configure Prompt Data

There are **two ways** to set the prompt data for your experiment:

- **Option 1**: Add prompts and variables through the console UI
  - Ideal for quick tests
- **Option 2**: Use datasets from past experiments or create new ones
  - Ideal for real, fully-configured experiments

### Step 4, Option 1: Add Prompt & Variables

In the Editor section, **add your prompt**.

In your prompt, you can use **variable names with curly brackets** (e.g. `{{variable_name}}`). This is great for testing how changing individual words in a prompt structure affects outputs.

Add new variable options with the **new tab icon** next to "VARIABLE SET 1 OF 1", or delete them with the **trash can icon**.

Add new messages beyond the initial prompt with the "+ Add Message" button **below the prompt field**. New messages can be from the user or the model ("system").

![Add Variables](/images/console-ui/variables-example.png)

### Step 4, Option 2: Add Dataset

Click the "Add Dataset" button to **choose a dataset** to be used by your model.

The datasets listed are from your past experiments. You can also [add your own](/getting-started/experiments/datasets) by clicking "Create new dataset".

[Learn more about datasets ‚Üí](/getting-started/experiments/datasets)

![Add Dataset](/images/console-ui/dataset-select.png)

### Step 5: Add Metrics

Click the "+ Add Metric" button to **choose metrics** by which your experiment's outputs are measured.

Filter and select from the preset metrics, or add your own by clicking "+ Create New Metric" in the top-right.

Scores are produced for each selected metric after running an experiment.

[Learn more about metrics ‚Üí](/concepts/metrics/overview)

![Add Metrics](/images/console-ui/metrics-select.png)

### Step 6: Add More Models, Prompts, and Settings

Add **additional prompt sections** with the "+ Compare Prompt" button in the top-right.

Each new prompt section can have its own distinct configuration of:

- Model
- Model settings
- Prompt
- Message conversation

Add new prompt sections and **customize their settings** as needed for your experiment.

![Add More Adjustments](/images/console-ui/two-models-example.png)

### Step 7: Run Experiments

Click the **"Run All" button in the top-right** to run your experiments, generate outputs, and calculate evaluations based on your chosen metrics.

![Run Experiments](/images/console-ui/generating-results.png)

### Step 8: Review Outputs

After the experiment has completed, **scroll down** to view their outputs and evaluations.

The more distinct prompts and variable sets you used, the more results there will be.

![Review Outputs](/images/console-ui/experiment-results.png)

### Step 9: Log Experiment Results

Click the "Log as Experiment" button above the outputs to **record all the details of the experiment**.

Use a **descriptive name** for your experiment so that it's easy to keep track of your progress.

[Learn more about logging ‚Üí](/getting-started/logging)

![Log Results](/images/console-ui/experiment-name.png)

### Step 10: Continue Experimenting!

That's it! Now, further customize and configure your experiment to meet your testing goals. Log your experiment results, and create new projects to try out different configurations.

If you encounter any errors, visit our [Common Errors guide](references/faqs/errors).

## ‚öôÔ∏è Experiment Settings & Options

![Continue Experimenting](/images/console-ui/labeled-console-ui.png)

1. **Model Select** - choose model to be used with prompt/dataset (and enter API keys if necessary)
2. **Model Settings** - adjust model-specific settings. [Learn more about model settings ‚Üí](#step-3-configure-model-settings)
3. **Message Originator** - select if the content of the prompt is from a user or from the model itself ("system").
4. **Prompt Entry** - add your prompt for the experiment. Use variables with curly brackets (e.g. `{{variable_name}}`) and add variable values in the variable entry field (#9). [Learn more about prompts & variables ‚Üí](#step-4-option-1-add-prompt--variables)
5. **Add Message** - use a multi-prompt conversation in your experiment by adding new messages.
6. **Dataset Select** - instead entering prompt(s), select a dataset of prompt data structures from a prior project, or add a new one. [Learn more about datasets ‚Üí](/getting-started/experiments/datasets)
7. **Metrics Select** - select metrics by which your experiment is evaluated. Select from Galileo's many presets, or create your own metrics. Scores are produced for each metric after running an experiment. [Learn more about metrics ‚Üí](/concepts/metrics/overview)
8. **Add Variable Set** - add new groups of values for the variables used in your prompt. This adds a new "VARIABLE SET" section along the bottom of the screen.
9. **Variable Value Entry** - set the values of the variables used in your prompt.
10. **Log Experiment** - record your experiment's prompt data, settings, metric evaluations, and outputs. [Learn more about logging ‚Üí](/getting-started/logging)
11. **Run Individual Experiment** - run your experiment using its individual prompt data and settings. When using multiple prompts, a "Run All" button appears in the top-right to run all of your experiments.
12. **Add Prompt Section** - add a new prompt section. Each prompt section can be configured with different models and prompts to compare and contrast their ouputs and metric evaluations. [Learn more about metrics ‚Üí](/concepts/metrics/overview)

## üß™ Examples of Common Experiment Scenarios

These examples illustrate how users typically run experiments in the Galileo Console to validate and improve AI application performance.

### Example 1: Validating a Sentiment Analysis Function

**Goal:** Ensure the model correctly classifies customer reviews as Positive, Neutral, or Negative.

**Setup:**

- **Function**: `analyze_sentiment` ‚Äì takes in review text, returns a label and optional confidence score.
- **Dataset**: 1,000 real customer reviews across different products.
- **Metrics**: Correctness (using exact label match), Token Highlighting enabled.

**Steps:**

1. Upload or select the review dataset.
2. Select the `analyze_sentiment` function.
3. Choose **Correctness** as the metric and enable token-level explainability.
4. Run the experiment.

**Expected Outcome:**
High accuracy in Positive/Negative samples. Token-level highlights reveal the influence of negation words and intensifiers.

### Example 2: Comparing Two LLMs on a Customer Support Task

**Goal:** Identify which model gives more helpful responses to customer queries.

**Setup:**

- **Functions**: `gpt-4` and `claude-3-haiku` versions of the same prompt template.
- **Dataset**: 100 frequently asked customer service questions.
- **Metrics**: Relevance and Completeness (measured via Galileo‚Äôs scoring UI or custom feedback rubric).

**Steps:**

1. Create one experiment per model with identical datasets.
2. Manually score Relevance and Completeness or configure custom metrics.
3. Use the Experiment Comparison view.

**Expected Outcome:**
Clear visual comparison of performance by metric. Decision support for production model selection.

### Example 3: Safety Evaluation of a Chatbot

**Goal:** Check for potential unsafe or toxic responses to edge-case prompts.

**Setup:**

- **Function**: `chatbot_response_v2`
- **Dataset**: Curated prompts designed to trigger edge behaviors (e.g., controversial, sensitive, or adversarial topics).
- **Metrics**: Safety (with Galileo's built-in Safety scoring model).

**Steps:**

1. Select the edge-case dataset.
2. Select the chatbot function.
3. Choose **Safety** as the metric.
4. Run the experiment and review flagged outputs.

**Expected Outcome:**
Identification of prompt types that lead to unsafe outputs. Use results to fine-tune safety guardrails or re-train the model.

### Example 4: Post-Deployment Regression Test

**Goal:** Validate that a recent model update did not degrade performance on core use cases.

**Setup:**

- **Function**: `qa_model_v3`
- **Dataset**: Golden set of key use-case inputs with known expected behavior.
- **Metrics**: Correctness, Token Highlighting.

**Steps:**

1. Run the latest model version on the test set.
2. Compare against the previous experiment‚Äôs results using Experiment Comparison.
3. Investigate any performance dips.

**Expected Outcome:**
Confidence in regression safety. Easy rollback if new issues are detected.
